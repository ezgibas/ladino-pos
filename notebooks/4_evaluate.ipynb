{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))  # Adjust the path accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from viterbi import Predictor\n",
    "from load_data import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../data/brown-universal.txt\"\n",
    "tags_file = \"../data/tags-universal.txt\"\n",
    "NLTK_model = \"../results/hmm_tagger-NLTK.pkl\"\n",
    "BW_model = \"../results/hmm_tagger-SS.pkl\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With NLTK Trained HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NLTK_model, 'rb') as pickle_file:\n",
    "    hmm_tagger_NLTK = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model parameters\n",
    "tags_nltk = hmm_tagger_NLTK._states  # Set of all possible tags\n",
    "words = hmm_tagger_NLTK._symbols  # Set of all possible words\n",
    "\n",
    "# convert probability distributions of HMM to dictionaries\n",
    "transitions = {}\n",
    "for prev_state in hmm_tagger_NLTK._transitions:\n",
    "    transitions[prev_state] = {}\n",
    "    for next_state in tags_nltk:\n",
    "        transitions[prev_state][next_state] = hmm_tagger_NLTK._transitions[prev_state].prob(next_state)\n",
    "\n",
    "emissions = {}\n",
    "for state in tags_nltk:\n",
    "    emissions[state] = {}\n",
    "    for word in words:\n",
    "        emissions[state][word] = hmm_tagger_NLTK._outputs[state].prob(word)\n",
    "\n",
    "initial = {}\n",
    "for state in tags_nltk:\n",
    "    initial[state] = hmm_tagger_NLTK._priors.prob(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLTK_tagger = Predictor(tags_nltk, transitions, emissions, initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df = pd.DataFrame([initial], columns=tags_nltk)\n",
    "initial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_df = pd.DataFrame.from_dict(emissions)\n",
    "print(sum(emissions_df.iloc[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_df = pd.DataFrame.from_dict(transitions)\n",
    "print(sum(transitions_df.iloc[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With HMM trained with own Baum-Welch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BW_model, 'rb') as pickle_file:\n",
    "    hmm_tagger_BW = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model parameters\n",
    "tags_bw = hmm_tagger_BW[\"states\"]  # Set of all possible tags\n",
    "words = hmm_tagger_BW[\"vocab\"]  # Set of all possible words\n",
    "\n",
    "# convert probability distributions of HMM to dictionaries\n",
    "transitions = {}\n",
    "transition_probs = hmm_tagger_BW[\"transition_probs\"]\n",
    "for prev_idx, prev_state in enumerate(tags_bw):\n",
    "    transitions[prev_state] = {}\n",
    "    for next_idx, next_state in enumerate(tags_bw):\n",
    "        transitions[prev_state][next_state] = transition_probs[prev_idx, next_idx]\n",
    "\n",
    "emissions = {}\n",
    "emission_probs = hmm_tagger_BW[\"emission_probs\"]\n",
    "for state_idx, state in enumerate(tags_bw):\n",
    "    emissions[state] = {}\n",
    "    for word_idx, word in enumerate(words):\n",
    "        emissions[state][word] = emission_probs[state_idx, word_idx]\n",
    "\n",
    "initial = {}\n",
    "initial_probs = hmm_tagger_BW[\"initial_probs\"]\n",
    "for state_idx, state in enumerate(tags_bw):\n",
    "    initial[state] = initial_probs[state_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BW_tagger = Predictor(tags_bw, transitions, emissions, initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df = pd.DataFrame([initial], columns=tags_bw)\n",
    "initial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_df = pd.DataFrame.from_dict(emissions)\n",
    "print(sum(emissions_df.iloc[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_df = pd.DataFrame.from_dict(transitions)\n",
    "print(sum(transitions_df.iloc[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_brown_data(data_file, split=0.8)\n",
    "\n",
    "test_sample = test # can split test to test on a smaller sample\n",
    "test_sample_sequences= [[token.get_word() for token in sentence] for sentence in test_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run prediction algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [[token.get_pos() for token in sentence] for sentence in test_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_nltk = [NLTK_tagger.viterbi(sequence) for sequence in test_sample_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_bw = [BW_tagger.viterbi(sequence) for sequence in test_sample_sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(actuals, targets):\n",
    "    \"\"\"Compute accuracy of guesses comparing to target data.\n",
    "    actual: list of guesses from the model ex. [['VERB', 'NOUN'], ['DET']]\n",
    "    target: list values from the test/validation set\n",
    "    \"\"\"\n",
    "    if len(actuals) != len(targets):\n",
    "        return -1 # the number of actual values should match number of target values\n",
    "    correct_count = 0\n",
    "    total_tags = 0\n",
    "    for actual_tags, target_tags in zip(actuals, targets):\n",
    "        total_tags += len(actual_tags)\n",
    "        if len(actual_tags) != len(target_tags):\n",
    "            return -1 # the number of actual values should match number of target values\n",
    "        for actual_value, target_value in zip(actual_tags, target_tags):\n",
    "            if actual_value == target_value:\n",
    "                correct_count += 1\n",
    "    \n",
    "    return correct_count/total_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate NLTK Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_accuracy(predictions_nltk, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate HMM trained with own Baum-Welch algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_accuracy(predictions_bw, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def create_hmm_confusion_matrix(true_states, predicted_states, title=\"HMM States Confusion Matrix\", \n",
    "                               state_names=None, normalize=True):\n",
    "    \"\"\"\n",
    "    Create and visualize a confusion matrix for HMM state predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_states : list or array\n",
    "        The true hidden state sequences from your labeled test data\n",
    "    predicted_states : list or array\n",
    "        The Viterbi-decoded state sequences from your model\n",
    "    title : str\n",
    "        Title for the confusion matrix plot\n",
    "    state_names : list, optional\n",
    "        Names of the hidden states (if they have semantic meaning)\n",
    "    normalize : bool\n",
    "        Whether to normalize the confusion matrix by row\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cm : numpy.ndarray\n",
    "        The confusion matrix\n",
    "    \"\"\"\n",
    "    # Flatten sequences if they're nested\n",
    "    if isinstance(true_states[0], (list, np.ndarray)):\n",
    "        true_flat = np.concatenate(true_states)\n",
    "        pred_flat = np.concatenate(predicted_states)\n",
    "    else:\n",
    "        true_flat = np.array(true_states)\n",
    "        pred_flat = np.array(predicted_states)\n",
    "    \n",
    "    # Get unique states\n",
    "    unique_states = sorted(set(np.concatenate([np.unique(true_flat), np.unique(pred_flat)])))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(true_flat, pred_flat, labels=unique_states)\n",
    "    \n",
    "    # Normalize if requested\n",
    "    if normalize:\n",
    "        cm_plot = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "    else:\n",
    "        cm_plot = cm\n",
    "        fmt = 'd'\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if state_names is None:\n",
    "        state_names = [f\"State {i}\" for i in unique_states]\n",
    "    \n",
    "    sns.heatmap(cm_plot, annot=True, fmt=fmt, cmap='BuPu',\n",
    "                xticklabels=state_names, yticklabels=state_names)\n",
    "    plt.xlabel('Predicted States')\n",
    "    plt.ylabel('True States')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - targets: list of true state sequences from your test data\n",
    "# - predictions_nltk: predictions from your supervised model\n",
    "# - predictions_bw: predictions from your unsupervised model\n",
    "\n",
    "# Confusion matrices\n",
    "supervised_cm = create_hmm_confusion_matrix(\n",
    "    targets, \n",
    "    predictions_nltk,\n",
    "    state_names=tags_nltk,\n",
    "    title=\"Supervised HMM Confusion Matrix\"\n",
    ")\n",
    "\n",
    "unsupervised_cm = create_hmm_confusion_matrix(\n",
    "    targets, \n",
    "    predictions_bw,\n",
    "    state_names=tags_bw,\n",
    "    title=\"Unsupervised HMM Confusion Matrix\"\n",
    ")\n",
    "\n",
    "#  Detailed error analysis\n",
    "def analyze_errors(cm, true_states, pred_states):\n",
    "    \"\"\"Analyze which sequences have the most errors\"\"\"  \n",
    "    # Calculate errors per sequence\n",
    "    errors = []\n",
    "    for true_seq, pred_seq in zip(true_states, pred_states):\n",
    "        errors.append(sum(t != p for t, p in zip(true_seq, pred_seq)))\n",
    "    \n",
    "    # Find most problematic sequences\n",
    "    sorted_indices = np.argsort(errors)[::-1]\n",
    "    \n",
    "    print(\"Top 5 most problematic sequences:\")\n",
    "    for i in sorted_indices[:5]:\n",
    "        print(f\"Sequence {i}: {errors[i]} errors\")\n",
    "        print(f\"  True:      {[s for s in true_states[i]]}\")\n",
    "        print(f\"  Predicted: {[s for s in pred_states[i]]}\")\n",
    "        sequence = \" \".join(test_sample[i])\n",
    "        print(f\"{sequence}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
