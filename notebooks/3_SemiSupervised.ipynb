{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69869499",
   "metadata": {},
   "source": [
    "This file should look very similar to notebooks 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad583eb",
   "metadata": {},
   "source": [
    "Main idea: Train with NLTK to get good initial probabilities for EM, fine-tune on data with EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))  # Adjust the path accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f2af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from load_data import *\n",
    "from preprocessing import *\n",
    "from baum_welch import *\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d81ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../data/brown-universal.txt\"\n",
    "tags_file = \"../data/tags-universal.txt\"\n",
    "model_file = \"../results/hmm_tagger-SS.pkl\"\n",
    "NLTK_model = \"../results/hmm_tagger-NLTK.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8316f44",
   "metadata": {},
   "source": [
    "# Load Data From File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_brown_data(data_file, split=0.8)\n",
    "tags = load_tags(tags_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec2bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} sentences in the training set.\".format(len(train)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition to train supervised HMM\n",
    "sup_train_sample = train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd315f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition to train unsupervised HMM (should be a superset of the previous)\n",
    "unsup_train_sample = train[10000:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91aefb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "unsup_train_sentences = []\n",
    "for sentence in unsup_train_sample:\n",
    "    train_sentence = []\n",
    "    for token in sentence:\n",
    "        word = token.get_word()\n",
    "        if word == '``' or word == \"''\":\n",
    "            word = '\"'\n",
    "        words.append(word)\n",
    "        train_sentence.append(word)\n",
    "    unsup_train_sentences.append(train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lookup = {word: i for i, word in enumerate(words)}\n",
    "tags_lookup = {tag: i for i, tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388845ad",
   "metadata": {},
   "source": [
    "# Semi-Supervised Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9441f6",
   "metadata": {},
   "source": [
    "## 1 Train HMM with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NLTK_model, 'rb') as pickle_file:\n",
    "    hmm_tagger_NLTK = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110e54e",
   "metadata": {},
   "source": [
    "### Extract Features of NLTK-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4213a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model parameters\n",
    "tags_nltk = hmm_tagger_NLTK._states  # Set of all possible tags\n",
    "words_nltk = hmm_tagger_NLTK._symbols  # Set of all possible words\n",
    "\n",
    "# convert probability distributions of HMM to dictionaries\n",
    "transitions_nltk = np.zeros((len(tags), len(tags)))\n",
    "for prev_state in hmm_tagger_NLTK._transitions:\n",
    "    i = tags_lookup[prev_state]\n",
    "    for next_state in tags_nltk:\n",
    "        j = tags_lookup[next_state]\n",
    "        transitions_nltk[i][j] = hmm_tagger_NLTK._transitions[prev_state].prob(next_state)\n",
    "\n",
    "\n",
    "emissions_nltk = np.zeros((len(tags), len(words)))\n",
    "for state in tags_nltk:\n",
    "    i = tags_lookup[state]\n",
    "    rand_emission_prob = hmm_tagger_NLTK._outputs[state].prob(hmm_tagger_NLTK._outputs[state].generate())\n",
    "    for word in words:\n",
    "        k = words_lookup[word]\n",
    "        if word in words_nltk:\n",
    "            emissions_nltk[i][k] = hmm_tagger_NLTK._outputs[state].prob(word)\n",
    "        else: \n",
    "            emissions_nltk[i][k] = rand_emission_prob # unseen word just as likely as some random emission\n",
    "\n",
    "# normalize emission probabilities\n",
    "row_sums = emissions_nltk.sum(axis=1)\n",
    "emissions = emissions_nltk / row_sums[:, np.newaxis]\n",
    "\n",
    "initial_nltk = np.zeros(len(tags))\n",
    "for state in tags_nltk:\n",
    "    i = tags_lookup[state]\n",
    "    initial_nltk[i] = hmm_tagger_NLTK._priors.prob(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed982c0",
   "metadata": {},
   "source": [
    "Verify that the initial probabilities are sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_row_sums = transitions_nltk.sum(axis=1)\n",
    "emissions_row_sums = emissions_nltk.sum(axis=1)\n",
    "initials_sum = sum(initial_nltk)\n",
    "print(trans_row_sums)\n",
    "print(emissions_row_sums)\n",
    "print(initials_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255ff98a",
   "metadata": {},
   "source": [
    "## 2 Train HMM with Baum-welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_tagger_500 = BaumWelch(tags, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_tagger_500.initialize_probabilities(transitions_nltk, emissions_nltk, initial_nltk, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34efb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(unsup_train_sentences)\n",
    "batch_size = 500\n",
    "batch_start = 0\n",
    "batch_end = batch_size\n",
    "done = False\n",
    "batch_logprobs = []\n",
    "while not done:\n",
    "    (bw_tagger_500, logprobs) = bw_tagger_500.train_em(unsup_train_sentences[batch_start:batch_end], max_iterations=20)\n",
    "    batch_logprobs.append(logprobs)\n",
    "    batch_start = batch_end\n",
    "    if batch_start >= num_samples:\n",
    "        done = True\n",
    "    else:\n",
    "        batch_end = min(batch_start + batch_size, num_samples)\n",
    "    if batch_end == num_samples:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_tagger_500.save_hmm(filename=\"../results/hmm_tagger-BW-500.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i, arr in enumerate(batch_logprobs):\n",
    "    inverted_values = [-1 * val for val in arr]\n",
    "    plt.plot(inverted_values, label=f'Batch {i+1}')\n",
    "\n",
    "# add labels and title\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Negative Log Probability')\n",
    "plt.title('Log Probability by Iteration by Batch')\n",
    "\n",
    "# add legend to distinguish between lines\n",
    "plt.legend()\n",
    "\n",
    "# add grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a94879",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_tagger_1000 = BaumWelch(tags, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_tagger_1000.initialize_probabilities(transitions_nltk, emissions_nltk, initial_nltk, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ee08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(unsup_train_sentences)\n",
    "batch_size = 100\n",
    "batch_start = 0\n",
    "batch_end = batch_size\n",
    "done = False\n",
    "batch_logprobs = []\n",
    "while not done:\n",
    "    (bw_tagger_1000, logprobs) = bw_tagger_1000.train_em(unsup_train_sentences[batch_start:batch_end], max_iterations=20)\n",
    "    batch_logprobs.append(logprobs)\n",
    "    batch_start = batch_end\n",
    "    if batch_start >= num_samples:\n",
    "        done = True\n",
    "    else:\n",
    "        batch_end = min(batch_start + batch_size, num_samples)\n",
    "    if batch_end == num_samples:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ad2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_tagger_1000.save_hmm(filename=\"../results/hmm_tagger-BW-500.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i, arr in enumerate(batch_logprobs):\n",
    "    inverted_values = [-1 * val for val in arr]\n",
    "    plt.plot(inverted_values, label=f'Batch {i+1}')\n",
    "\n",
    "# add labels and title\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Negative Log Probability')\n",
    "plt.title('Log Probability by Iteration by Batch')\n",
    "\n",
    "# add legend to distinguish between lines\n",
    "plt.legend()\n",
    "\n",
    "# add grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
