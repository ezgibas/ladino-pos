{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))  # Adjust the path accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_preprocessed_data import load_ladino_pos\n",
    "\n",
    "weak_dataset_file_path = '../data/weak/ladino-pos.txt'\n",
    "weak_tags, weak_tags_dict = load_ladino_pos(weak_dataset_file_path) # import ladino tokens into custom data definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check it worked\n",
    "for item in weak_tags[0]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "labeled_data = weak_tags\n",
    "unlabeled_data = load_dataset(\"collectivat/salom-ladino-articles\")['train']\n",
    "train = unlabeled_data[:10300]['text']\n",
    "validate = unlabeled_data[10301:10500]['text']\n",
    "test = unlabeled_data[10501:10685]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count tags and emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results = False\n",
    "\n",
    "\"\"\"\n",
    "Count tags, tag transitions, and emissions of words to create the proper probability tables:\n",
    "P(Tag)\n",
    "P(Tag_{i} | Tag_{i-1})\n",
    "P(Word | Tag)\n",
    "\"\"\"\n",
    "def create_count_dictionaries(data):\n",
    "    tag_counts = {} # P(Tag)\n",
    "    tag_transition_counts = {} # P(Tag_{i} | Tag_{i-1})\n",
    "    emission_counts = {} # P(Word | Tag)\n",
    "    # go through each sentence in the data\n",
    "    for sentence in data:\n",
    "        tags_sequence = [word.get_pos() for word in sentence]\n",
    "        words_sequence = [word.get_word() for word in sentence]\n",
    "        prev_tag = \"<s>\" # all sentences start with delimiter\n",
    "        # go through each word and tag\n",
    "        for word, tag in zip(words_sequence, tags_sequence):\n",
    "            # P(Tag)\n",
    "            tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
    "\n",
    "            # P(Tag_{i} | Tag_{i-1})\n",
    "            tag_transition = (prev_tag, tag) # make key to indicate transitioning from the previous tag to current\n",
    "            tag_transition_counts[tag_transition] = tag_transition_counts.get(tag_transition, 0) + 1\n",
    "            prev_tag = tag\n",
    "\n",
    "            # P(Word | Tag)\n",
    "            emission = (tag, word)\n",
    "            emission_counts[emission] = emission_counts.get(emission, 0) + 1\n",
    "        \n",
    "        # P(Tag_{i} | Tag_{i-1}) only for the end of the sentence\n",
    "        tag_transition = (prev_tag, \"<s/>\") # all sentences end with delimiter\n",
    "        tag_transition_counts[tag_transition] = tag_transition_counts.get(tag_transition, 0) + 1\n",
    "    return tag_counts, tag_transition_counts, emission_counts\n",
    "\n",
    "tag_counts, tag_transition_counts, emission_counts = create_count_dictionaries(weak_tags)\n",
    "\n",
    "if print_results:\n",
    "    tag_counts = sorted(tag_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    tag_transition_counts = sorted(tag_transition_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    emission_counts = sorted(emission_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    print(tag_counts)\n",
    "    print(tag_transition_counts)\n",
    "    print(emission_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make actual probability tables out of counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print_results = True\n",
    "\n",
    "tags_matrix = np.zeros((len(tag_counts), len(tag_counts)), dtype='float32')\n",
    "tags_total_count = sum(tag_transition_counts.values())\n",
    "tags = list(tag_counts.keys())\n",
    "for i in range(len(tag_counts)):\n",
    "    for j in range(len(tag_counts)):\n",
    "        count_of_transition = tag_transition_counts.get((tags[i], tags[j]), 0)\n",
    "        tags_matrix[i, j] = count_of_transition/tag_counts.get(tags[i])\n",
    "\n",
    "\n",
    "# need to create table for emission probabilities too? TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_matrix_df = pd.DataFrame(tags_matrix, columns = list(tag_counts), index=list(tag_counts))\n",
    "tags_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create HMM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
