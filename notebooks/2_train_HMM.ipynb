{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))  # Adjust the path accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_preprocessed_data import load_ladino_pos\n",
    "\n",
    "weak_dataset_file_path = '../data/weak/ladino-pos.txt'\n",
    "weak_tags, weak_tags_dict = load_ladino_pos(weak_dataset_file_path) # import ladino tokens into custom data definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check it worked\n",
    "print(weak_tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "labeled_data = weak_tags\n",
    "unlabeled_data = load_dataset(\"collectivat/salom-ladino-articles\")['train']\n",
    "train = unlabeled_data[:10300]['text']\n",
    "validate = unlabeled_data[10301:10500]['text']\n",
    "test = unlabeled_data[10501:10685]['text']\n",
    "\n",
    "train = [sentence.split() for sentence in train]\n",
    "validate = [sentence.split() for sentence in validate]\n",
    "test = [sentence.split() for sentence in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count tags and emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results = False\n",
    "\n",
    "\"\"\"\n",
    "Count tags, tag transitions, and emissions of words to create the proper probability tables:\n",
    "P(Tag)\n",
    "P(Tag_{i} | Tag_{i-1})\n",
    "P(Word | Tag)\n",
    "\"\"\"\n",
    "def create_count_dictionaries(data):\n",
    "    tag_counts = {} # P(Tag)\n",
    "    tag_transition_counts = {} # P(Tag_{i} | Tag_{i-1})\n",
    "    # emission_counts = {} # P(Word | Tag)\n",
    "    # go through each sentence in the data\n",
    "    for sentence in data:\n",
    "        tags_sequence = [word.get_pos() for word in sentence]\n",
    "        words_sequence = [word.get_word() for word in sentence]\n",
    "        prev_tag = \"<s>\" # all sentences start with delimiter\n",
    "        # go through each word and tag\n",
    "        for _, tag in zip(words_sequence, tags_sequence):\n",
    "            # P(Tag)\n",
    "            tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
    "\n",
    "            # P(Tag_{i} | Tag_{i-1})\n",
    "            tag_transition = (prev_tag, tag) # make key to indicate transitioning from the previous tag to current\n",
    "            tag_transition_counts[tag_transition] = tag_transition_counts.get(tag_transition, 0) + 1\n",
    "            prev_tag = tag\n",
    "        \n",
    "        # P(Tag_{i} | Tag_{i-1}) only for the end of the sentence\n",
    "        tag_transition = (prev_tag, \"<s/>\") # all sentences end with delimiter\n",
    "        tag_transition_counts[tag_transition] = tag_transition_counts.get(tag_transition, 0) + 1\n",
    "    return tag_counts, tag_transition_counts\n",
    "\n",
    "tag_counts, tag_transition_counts = create_count_dictionaries(weak_tags)\n",
    "\n",
    "if print_results:\n",
    "    tag_counts = sorted(tag_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    tag_transition_counts = sorted(tag_transition_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    # emission_counts = sorted(emission_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    print(tag_counts)\n",
    "    print(tag_transition_counts)\n",
    "    # print(emission_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make actual probability tables out of counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a matrix for Tag -> Tag transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results = True\n",
    "\n",
    "tags_matrix = np.zeros((len(tag_counts), len(tag_counts)), dtype='float32')\n",
    "\n",
    "tags = sorted({tag for counter in weak_tags_dict.values() for tag in counter}) # columns\n",
    "tag_to_index = {tag: j for j, tag in enumerate(tags)}\n",
    "\n",
    "for tag_1 in tags:\n",
    "    for tag_2 in tags:\n",
    "        i = tag_to_index[tag_1]\n",
    "        j = tag_to_index[tag_2]\n",
    "        count_of_transition = tag_transition_counts.get((tag_1, tag_2), 0)\n",
    "        tags_matrix[i, j] = count_of_transition/tag_counts.get(tag_1)\n",
    "\n",
    "# need to create table for emission probabilities too? TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_matrix_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\n",
    "tags_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a matrix for Tag -> Word probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(weak_tags_dict.keys())  # rows\n",
    "# columns are \"tags\" defined in previous cell\n",
    "\n",
    "# create mapping of words and tags to an index so that we can\n",
    "# add to the correct tag/word every time we are updating the matrix5\n",
    "word_to_index = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "emission_matrix = np.zeros((len(words), len(tags)))\n",
    "\n",
    "for word, counter in weak_tags_dict.items():\n",
    "    for tag, count in counter.items():\n",
    "        emission_matrix[word_to_index[word], tag_to_index[tag]] = count\n",
    "\n",
    "\n",
    "emission_matrix = emission_matrix / emission_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "# due to processing of data, some rows are NaN, replace them \n",
    "# so they don't affect later calculations\n",
    "# TODO fix data processing so this doesn't happen\n",
    "emission_matrix = np.nan_to_num(emission_matrix, nan=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ems_matrix_df = pd.DataFrame(emission_matrix, columns = list(tags), index=list(words))\n",
    "ems_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create initial probabilities matrix (the probability a sentence starts with a tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_probs = np.zeros(len(tags))\n",
    "for i in range(len(tags)):\n",
    "    prob = tag_transition_counts.get(('<s>', tags[i]), 0)\n",
    "    initial_probs[i] = prob\n",
    "\n",
    "initial_probs = initial_probs / initial_probs.sum()\n",
    "\n",
    "print(tags)\n",
    "print(initial_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_probs_df = pd.DataFrame([initial_probs], columns = tags)\n",
    "initial_probs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmm\n",
    "\n",
    "hmm_tagger = hmm.HMMTagger(tags, words, smoothing=2.0)\n",
    "hmm_tagger.initialize_probabilities(tags_matrix, emission_matrix, initial_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_tagger.train_em(train[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hmm_tagger.viterbi(test[0])\n",
    "print(test[0])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
