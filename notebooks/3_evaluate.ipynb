{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))  # Adjust the path accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from load_data import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../data/brown-universal.txt\"\n",
    "tags_file = \"../data/tags-universal.txt\"\n",
    "NLTK_model = \"../results/hmm_tagger-NLTK.pkl\"\n",
    "BW_model = \"../results/hmm_tagger-BW.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, tags, transitions, emissions, initial):\n",
    "        self.tags = tags\n",
    "        self.transitions = transitions\n",
    "        self.emissions = emissions\n",
    "        self.initial = initial\n",
    "\n",
    "    def viterbi(self, sequence):\n",
    "        \"\"\"\n",
    "        Predict the Part of Speech tags for a given sentence using the Viterbi algorithm\n",
    "        \"\"\"\n",
    "        tags = self.tags\n",
    "        transitions = self.transitions\n",
    "        emissions = self.emissions\n",
    "        initial = self.initial\n",
    "\n",
    "        # viterbi matrix\n",
    "        # V[t][i] = value of path with the highest probability that accounts for the first t observations\n",
    "        V = [{}] \n",
    "        # path matrix\n",
    "        # path[t][i] = path w/ highest probability that accounts for first t observations\n",
    "        path = [{}]\n",
    "\n",
    "        # i.e. V is the max(), path is the argmax() \n",
    "        \n",
    "        # initialize first step\n",
    "        for state in tags:\n",
    "            # handle OOV words w/ small probability\n",
    "            emission_prob = emissions[state].get(sequence[0], 1e-5)\n",
    "            V[0][state] = initial[state] * emission_prob\n",
    "            path[0][state] = [state]\n",
    "        \n",
    "        # recursion\n",
    "        for t in range(1, len(sequence)):\n",
    "            V.append({})\n",
    "            path.append({})\n",
    "            \n",
    "            for cur_state in tags:\n",
    "                # handle OOV\n",
    "                emission_prob = emissions[cur_state].get(sequence[t], 1e-5)\n",
    "                \n",
    "                # initialize max, argmax to nothing\n",
    "                max_prob = float('-inf')\n",
    "                max_state = None\n",
    "                \n",
    "                # get max, argmax of V[t-1][i]*transitions[i][j] over all states i\n",
    "                for prev_state in tags: # for all states i\n",
    "                    # smoothing for missing transitions\n",
    "                    transition_prob = transitions[prev_state].get(cur_state, 1e-5)\n",
    "\n",
    "                    # V[t-1][i]*transitions[i][j]\n",
    "                    prob = V[t-1][prev_state] * transition_prob * emission_prob\n",
    "                    \n",
    "                    # max, argmax\n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        max_state = prev_state\n",
    "                \n",
    "                # V[t][j] = max(V[t-1][i]*transitions[i][j])*emissions[j][observation_t]\n",
    "                V[t][cur_state] = max_prob\n",
    "                # path[t][j] = argmax(V[t-1][i]*transitions[i][j])*emissions[j][observation_t]\n",
    "                path[t][cur_state] = path[t-1][max_state] + [cur_state]\n",
    "                    \n",
    "\n",
    "        # termination + backtracking\n",
    "        T = len(sequence)-1 # T = last time-step\n",
    "\n",
    "        best_final_state = max(V[T], key=V[T].get)\n",
    "\n",
    "        best_path = path[T][best_final_state] # path stores completes paths, so just access the last one\n",
    "\n",
    "        return best_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With NLTK Trained HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NLTK_model, 'rb') as pickle_file:\n",
    "    hmm_tagger_NLTK = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model parameters\n",
    "tags = hmm_tagger_NLTK._states  # Set of all possible tags\n",
    "words = hmm_tagger_NLTK._symbols  # Set of all possible words\n",
    "\n",
    "# convert probability distributions of HMM to dictionaries\n",
    "transitions = {}\n",
    "for prev_state in hmm_tagger_NLTK._transitions:\n",
    "    transitions[prev_state] = {}\n",
    "    for next_state in tags:\n",
    "        transitions[prev_state][next_state] = hmm_tagger_NLTK._transitions[prev_state].prob(next_state)\n",
    "\n",
    "emissions = {}\n",
    "for state in tags:\n",
    "    emissions[state] = {}\n",
    "    for word in words:\n",
    "        emissions[state][word] = hmm_tagger_NLTK._outputs[state].prob(word)\n",
    "\n",
    "initial = {}\n",
    "for state in tags:\n",
    "    initial[state] = hmm_tagger_NLTK._priors.prob(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLTK_tagger = Predictor(tags, transitions, emissions, initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df = pd.DataFrame([initial], columns=tags)\n",
    "initial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_df = pd.DataFrame.from_dict(emissions)\n",
    "print(sum(emissions_df.iloc[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_df = pd.DataFrame.from_dict(transitions)\n",
    "print(sum(transitions_df.iloc[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With HMM trained with own Baum-Welch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BW_model, 'rb') as pickle_file:\n",
    "    hmm_tagger_BW = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model parameters\n",
    "tags = hmm_tagger_BW[\"states\"]  # Set of all possible tags\n",
    "words = hmm_tagger_BW[\"vocab\"]  # Set of all possible words\n",
    "\n",
    "# convert probability distributions of HMM to dictionaries\n",
    "transitions = {}\n",
    "transition_probs = hmm_tagger_BW[\"transition_probs\"]\n",
    "for prev_idx, prev_state in enumerate(tags):\n",
    "    transitions[prev_state] = {}\n",
    "    for next_idx, next_state in enumerate(tags):\n",
    "        transitions[prev_state][next_state] = transition_probs[prev_idx, next_idx]\n",
    "\n",
    "emissions = {}\n",
    "emission_probs = hmm_tagger_BW[\"emission_probs\"]\n",
    "for state_idx, state in enumerate(tags):\n",
    "    emissions[state] = {}\n",
    "    for word_idx, word in enumerate(words):\n",
    "        emissions[state][word] = emission_probs[state_idx, word_idx]\n",
    "\n",
    "initial = {}\n",
    "initial_probs = hmm_tagger_BW[\"initial_probs\"]\n",
    "for state_idx, state in enumerate(tags):\n",
    "    initial[state] = initial_probs[state_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BW_tagger = Predictor(tags, transitions, emissions, initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df = pd.DataFrame([initial], columns=tags)\n",
    "initial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_df = pd.DataFrame.from_dict(emissions)\n",
    "print(sum(emissions_df.iloc[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_df = pd.DataFrame.from_dict(transitions)\n",
    "print(sum(transitions_df.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import predictions from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(actuals, targets):\n",
    "    \"\"\"Compute accuracy of guesses comparing to target data.\n",
    "    actual: list of guesses from the model ex. [['VERB', 'NOUN'], ['DET']]\n",
    "    target: list values from the test/validation set\n",
    "    \"\"\"\n",
    "    if len(actuals) != len(targets):\n",
    "        return -1 # the number of actual values should match number of target values\n",
    "    correct_count = 0\n",
    "    total_tags = 0\n",
    "    for actual_tags, target_tags in zip(actuals, targets):\n",
    "        total_tags += len(actual_tags)\n",
    "        if len(actual_tags) != len(target_tags):\n",
    "            return -1 # the number of actual values should match number of target values\n",
    "        for actual_value, target_value in zip(actual_tags, target_tags):\n",
    "            if actual_value == target_value:\n",
    "                correct_count += 1\n",
    "    \n",
    "    return correct_count/total_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_brown_data(data_file, split=0.8)\n",
    "\n",
    "test_sample = test # can split test to test on a smaller sample\n",
    "test_sample = [[token.get_word() for token in sentence] for sentence in test_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate NLTK Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [NLTK_tagger.viterbi(sequence) for sequence in test_sample]\n",
    "targets = [[token.get_pos() for token in sentence] for sentence in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_accuracy(predictions, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate HMM trained with own Baum-Welch algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [BW_tagger.viterbi(sequence) for sequence in test_sample]\n",
    "targets = [[token.get_pos() for token in sentence] for sentence in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_accuracy(predictions, targets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
