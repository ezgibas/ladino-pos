{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5db9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))  # Adjust the path accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5bc6115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from load_data import *\n",
    "from viterbi import *\n",
    "from preprocessing import *\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from factored_hmm import FactoredHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "972ba7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../data/brown-universal.txt\"\n",
    "tags_file = \"../data/tags-universal.txt\"\n",
    "model_file = \"../results/hmm_tagger-SS.pkl\"\n",
    "NLTK_model = \"../results/hmm_tagger-NLTK.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce898212",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_brown_data(data_file, split=0.8)\n",
    "tags = load_tags(tags_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659ee228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45872 sentences in the training set.\n",
      "There are 11468 sentences in the testing set.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} sentences in the training set.\".format(len(train)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9df950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition to train unsupervised HMM (should be a superset of the previous)\n",
    "train = train\n",
    "test = test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "193d9e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_train = train[:10000]\n",
    "unsup_train = train[10000:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7abfdd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for sentence in train:\n",
    "    for token in sentence:\n",
    "        word = token.get_word()\n",
    "        # if word == '``' or word == \"''\":\n",
    "        #     word = '\"'\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af2d6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_idx = {tag: i for i, tag in enumerate(tags)}\n",
    "idx_to_tag = {idx: tag for tag, idx in tag_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b34c2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string tags to integers\n",
    "int_training_data = []\n",
    "for sentence in sup_train:\n",
    "    int_sentence = [(token.get_word(), tag_to_idx[token.get_pos()]) for token in sentence]\n",
    "    int_training_data.append(int_sentence)\n",
    "\n",
    "int_test_data = []\n",
    "for sentence in test:\n",
    "    int_sentence = [(token.get_word(), tag_to_idx[token.get_pos()]) for token in sentence]\n",
    "    int_test_data.append(int_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1412168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for sentence in unsup_train:\n",
    "    s = [token.get_word() for token in sentence]\n",
    "    sequences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "797c96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train the factored HMM\n",
    "num_pos_tags = len(tag_to_idx)\n",
    "states_per_tag = 3  # Using 3 states per POS tag\n",
    "\n",
    "# initialize the model\n",
    "hmm = FactoredHMM(num_pos_tags, states_per_tag, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de0f42ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.initialize_from_tagged_data(int_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "667669c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 logprob -375.31145158955746\n",
      "iteration 1 logprob -162.9105377276921\n",
      "iteration 2 logprob -158.39508547507322\n",
      "iteration 3 logprob -157.3659213761136\n",
      "iteration 4 logprob -157.04780893407343\n",
      "iteration 5 logprob -156.9294278348261\n",
      "iteration 6 logprob -156.87896333734219\n",
      "iteration 7 logprob -156.8550330254664\n",
      "iteration 8 logprob -156.84266060557786\n",
      "iteration 9 logprob -156.83578858614396\n",
      "iteration 10 logprob -156.83173446860732\n",
      "iteration 11 logprob -156.82921693244157\n",
      "iteration 12 logprob -156.82758329767347\n",
      "iteration 13 logprob -156.82648219088213\n",
      "iteration 14 logprob -156.82571513026403\n",
      "iteration 0 logprob -356.7431906496996\n",
      "iteration 1 logprob -181.90721328354203\n",
      "iteration 2 logprob -179.96846393447635\n",
      "iteration 3 logprob -179.5411789075788\n",
      "iteration 4 logprob -179.4106203603169\n",
      "iteration 5 logprob -179.36223601698995\n",
      "iteration 6 logprob -179.34164467149765\n",
      "iteration 7 logprob -179.33188751289956\n",
      "iteration 8 logprob -179.3268447265735\n",
      "iteration 9 logprob -179.3240443549587\n",
      "iteration 10 logprob -179.32239247119335\n",
      "iteration 11 logprob -179.32136674869236\n",
      "iteration 12 logprob -179.32070118317864\n",
      "iteration 0 logprob -304.25838884074483\n",
      "iteration 1 logprob -157.10768550638062\n",
      "iteration 2 logprob -155.36367871926865\n",
      "iteration 3 logprob -154.9743725329761\n",
      "iteration 4 logprob -154.8548957513237\n",
      "iteration 5 logprob -154.81054757713318\n",
      "iteration 6 logprob -154.79166178204096\n",
      "iteration 7 logprob -154.7827102097341\n",
      "iteration 8 logprob -154.77808311681517\n",
      "iteration 9 logprob -154.7755133947174\n",
      "iteration 10 logprob -154.7739975013172\n",
      "iteration 11 logprob -154.77305619643238\n",
      "iteration 0 logprob -337.86354245505527\n",
      "iteration 1 logprob -176.28487991991918\n",
      "iteration 2 logprob -174.3847922517102\n",
      "iteration 3 logprob -173.96323608617618\n",
      "iteration 4 logprob -173.83415461085332\n",
      "iteration 5 logprob -173.78628177083365\n",
      "iteration 6 logprob -173.7659019996037\n",
      "iteration 7 logprob -173.75624380112592\n",
      "iteration 8 logprob -173.75125183187777\n",
      "iteration 9 logprob -173.74847958312174\n",
      "iteration 10 logprob -173.74684425593153\n",
      "iteration 11 logprob -173.74582880192207\n",
      "iteration 12 logprob -173.74516989438416\n",
      "iteration 0 logprob -314.5151363702916\n",
      "iteration 1 logprob -161.59990272348742\n",
      "iteration 2 logprob -159.8016070094955\n",
      "iteration 3 logprob -159.40189119473405\n",
      "iteration 4 logprob -159.2794111474067\n",
      "iteration 5 logprob -159.23397446500383\n",
      "iteration 6 logprob -159.2146296660518\n",
      "iteration 7 logprob -159.20546150030182\n",
      "iteration 8 logprob -159.20072269581354\n",
      "iteration 9 logprob -159.19809100591104\n",
      "iteration 10 logprob -159.19653858180277\n",
      "iteration 11 logprob -159.1955746020557\n",
      "iteration 0 logprob -343.8663306232454\n",
      "iteration 1 logprob -178.03146208696933\n",
      "iteration 2 logprob -176.01862775842736\n",
      "iteration 3 logprob -175.5705560855644\n",
      "iteration 4 logprob -175.4331272741946\n",
      "iteration 5 logprob -175.38212441079835\n",
      "iteration 6 logprob -175.36040607548625\n",
      "iteration 7 logprob -175.35011220353692\n",
      "iteration 8 logprob -175.3447913419067\n",
      "iteration 9 logprob -175.34183634577732\n",
      "iteration 10 logprob -175.3400931837403\n",
      "iteration 11 logprob -175.3390107570759\n",
      "iteration 12 logprob -175.3383083868638\n",
      "iteration 0 logprob -341.1628374071388\n",
      "iteration 1 logprob -179.96978667554416\n",
      "iteration 2 logprob -178.0431281750491\n",
      "iteration 3 logprob -177.61436670309192\n",
      "iteration 4 logprob -177.48294775082985\n",
      "iteration 5 logprob -177.43419037393727\n",
      "iteration 6 logprob -177.4134309871569\n",
      "iteration 7 logprob -177.40359217077471\n",
      "iteration 8 logprob -177.39850656054617\n",
      "iteration 9 logprob -177.39568206241728\n",
      "iteration 10 logprob -177.3940155954776\n",
      "iteration 11 logprob -177.39298036344425\n",
      "iteration 12 logprob -177.39230802181038\n",
      "iteration 0 logprob -340.1343823862064\n",
      "iteration 1 logprob -182.371213413614\n",
      "iteration 2 logprob -180.53476561999622\n",
      "iteration 3 logprob -180.12763213818184\n",
      "iteration 4 logprob -180.00294780261183\n",
      "iteration 5 logprob -179.95667146001108\n",
      "iteration 6 logprob -179.93690845499003\n",
      "iteration 7 logprob -179.92742711132394\n",
      "iteration 8 logprob -179.92232396308995\n",
      "iteration 9 logprob -179.9191516579077\n",
      "iteration 10 logprob -179.91674277548006\n",
      "iteration 11 logprob -179.91443429619864\n",
      "iteration 12 logprob -179.91176831019638\n",
      "iteration 13 logprob -179.90837804019392\n",
      "iteration 14 logprob -179.90395839341008\n",
      "iteration 15 logprob -179.89827878385393\n",
      "iteration 16 logprob -179.8912102909748\n",
      "iteration 17 logprob -179.88274282633537\n",
      "iteration 18 logprob -179.8729778194305\n",
      "iteration 19 logprob -179.86209846200845\n",
      "iteration 20 logprob -179.8503310573973\n",
      "iteration 21 logprob -179.83791093576812\n",
      "iteration 22 logprob -179.82505960161458\n",
      "iteration 23 logprob -179.8119732935571\n",
      "iteration 24 logprob -179.79881977015256\n",
      "iteration 25 logprob -179.7857395764117\n",
      "iteration 26 logprob -179.77284900286094\n",
      "iteration 27 logprob -179.7602432343957\n",
      "iteration 28 logprob -179.7479991480254\n",
      "iteration 29 logprob -179.73617771424483\n",
      "iteration 30 logprob -179.7248261059445\n",
      "iteration 31 logprob -179.71397959395668\n",
      "iteration 32 logprob -179.70366324004462\n",
      "iteration 33 logprob -179.69389335683326\n",
      "iteration 34 logprob -179.6846787098934\n",
      "iteration 35 logprob -179.67602147614866\n",
      "iteration 36 logprob -179.66791801628924\n",
      "iteration 37 logprob -179.66035954213558\n",
      "iteration 38 logprob -179.6533327539515\n",
      "iteration 39 logprob -179.64682049498447\n",
      "iteration 40 logprob -179.64080243637778\n",
      "iteration 41 logprob -179.63525577835375\n",
      "iteration 42 logprob -179.63015593933892\n",
      "iteration 43 logprob -179.62547720275575\n",
      "iteration 44 logprob -179.62119329726957\n",
      "iteration 45 logprob -179.61727789563116\n",
      "iteration 46 logprob -179.61370502647935\n",
      "iteration 47 logprob -179.61044940080063\n",
      "iteration 48 logprob -179.60748665965676\n",
      "iteration 49 logprob -179.60479355251937\n",
      "iteration 50 logprob -179.6023480566093\n",
      "iteration 51 logprob -179.60012944755542\n",
      "iteration 52 logprob -179.5981183309446\n",
      "iteration 53 logprob -179.5962966432343\n",
      "iteration 54 logprob -179.5946476292981\n",
      "iteration 55 logprob -179.59315580268196\n",
      "iteration 56 logprob -179.5918068935534\n",
      "iteration 57 logprob -179.59058778835518\n",
      "iteration 58 logprob -179.58948646434652\n",
      "iteration 59 logprob -179.58849192151368\n",
      "iteration 0 logprob -323.3370969644794\n",
      "iteration 1 logprob -168.02502769538674\n",
      "iteration 2 logprob -165.92581873221275\n",
      "iteration 3 logprob -165.09473984257005\n",
      "iteration 4 logprob -164.17739197886644\n",
      "iteration 5 logprob -162.79135005156832\n",
      "iteration 6 logprob -160.86539500782123\n",
      "iteration 7 logprob -158.6094324362954\n",
      "iteration 8 logprob -156.27013587183146\n",
      "iteration 9 logprob -154.09736601608057\n",
      "iteration 10 logprob -152.13958399801012\n",
      "iteration 11 logprob -150.39410394098633\n",
      "iteration 12 logprob -148.91628510028167\n",
      "iteration 13 logprob -147.68064159643453\n",
      "iteration 14 logprob -146.64226295605638\n",
      "iteration 15 logprob -145.7597161613522\n",
      "iteration 16 logprob -144.998368954107\n",
      "iteration 17 logprob -144.34203736621322\n",
      "iteration 18 logprob -143.7784001725714\n",
      "iteration 19 logprob -143.29349559466544\n",
      "iteration 20 logprob -142.87516260882862\n",
      "iteration 21 logprob -142.51423303565105\n",
      "iteration 22 logprob -142.203053160816\n",
      "iteration 23 logprob -141.93465321375942\n",
      "iteration 24 logprob -141.70277656941892\n",
      "iteration 25 logprob -141.50203121773723\n",
      "iteration 26 logprob -141.3278540978758\n",
      "iteration 27 logprob -141.17636435796013\n",
      "iteration 28 logprob -141.0442443369349\n",
      "iteration 29 logprob -140.92866971142456\n",
      "iteration 30 logprob -140.82725632119602\n",
      "iteration 31 logprob -140.7380060562431\n",
      "iteration 32 logprob -140.6592509040638\n",
      "iteration 33 logprob -140.5895982025362\n",
      "iteration 34 logprob -140.52787994009776\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_logprobs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m----> 8\u001b[0m     (hmm, logprobs) \u001b[38;5;241m=\u001b[39m \u001b[43mhmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_em\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     batch_logprobs\u001b[38;5;241m.\u001b[39mappend(logprobs)\n\u001b[1;32m     10\u001b[0m     batch_start \u001b[38;5;241m=\u001b[39m batch_end\n",
      "File \u001b[0;32m~/cs4100/ladino-pos/scripts/baum_welch.py:135\u001b[0m, in \u001b[0;36mBaumWelch.train_em\u001b[0;34m(self, sequences, max_iterations, learning_rate, decay_rate)\u001b[0m\n\u001b[1;32m    133\u001b[0m         prob_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m logprob_ems_i[k]\n\u001b[1;32m    134\u001b[0m         interpolated_prob \u001b[38;5;241m=\u001b[39m  (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m learning_rate) \u001b[38;5;241m*\u001b[39m prob_old \u001b[38;5;241m+\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m prob_new\n\u001b[0;32m--> 135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memission_probs[i, k] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog2\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterpolated_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# initial probabilities\u001b[39;00m\n\u001b[1;32m    137\u001b[0m logprob_initial \u001b[38;5;241m=\u001b[39m acc_initial_num \u001b[38;5;241m-\u001b[39m acc_initial_denom\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_samples = len(sequences)\n",
    "batch_size = 100\n",
    "batch_start = 0\n",
    "batch_end = batch_size\n",
    "done = False\n",
    "batch_logprobs = []\n",
    "while not done:\n",
    "    (hmm, logprobs) = hmm.train_em(sequences[batch_start:batch_end], max_iterations=100)\n",
    "    batch_logprobs.append(logprobs)\n",
    "    batch_start = batch_end\n",
    "    if batch_start >= num_samples:\n",
    "        done = True\n",
    "    else:\n",
    "        batch_end = min(batch_start + batch_size, num_samples)\n",
    "    if batch_end == num_samples:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e406d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model parameters\n",
    "tags_bw = hmm.states  # Set of all possible tags\n",
    "words = hmm.vocab # Set of all possible words\n",
    "\n",
    "# convert probability distributions of HMM to dictionaries\n",
    "transitions = {}\n",
    "transition_probs = hmm.transition_probs\n",
    "for prev_idx, prev_state in enumerate(tags_bw):\n",
    "    transitions[prev_state] = {}\n",
    "    for next_idx, next_state in enumerate(tags_bw):\n",
    "        transitions[prev_state][next_state] = transition_probs[prev_idx, next_idx]\n",
    "\n",
    "emissions = {}\n",
    "emission_probs = hmm.emission_probs\n",
    "for state_idx, state in enumerate(tags_bw):\n",
    "    emissions[state] = {}\n",
    "    for word_idx, word in enumerate(words):\n",
    "        emissions[state][word] = emission_probs[state_idx, word_idx]\n",
    "\n",
    "initial = {}\n",
    "initial_probs = hmm.initial_probs\n",
    "for state_idx, state in enumerate(tags_bw):\n",
    "    initial[state] = initial_probs[state_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12c93c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = Predictor(tags_bw, transitions, emissions, initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "596a9fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [tagger.viterbi(sequence) for sequence in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50bacd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.update_state_to_tag_mapping(int_training_data, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32143f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.save_hmm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88be5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # use the model for prediction\n",
    "# new_sentence = [\"The\", \"cat\", \"jumps\", \"over\", \"the\", \"fence\", \".\"]\n",
    "# predicted_tag_indices = hmm.decode_to_pos_tags(new_sentence)\n",
    "\n",
    "# # Convert numeric tags back to readable format\n",
    "# predicted_tags = [idx_to_tag[idx] for idx in predicted_tag_indices]\n",
    "\n",
    "# # Print the results\n",
    "# print(\"\\nPrediction for new sentence:\")\n",
    "# for word, tag in zip(new_sentence, predicted_tags):\n",
    "#     print(f\"{word}: {tag}\")\n",
    "\n",
    "# # print  the state-to-tag mapping\n",
    "# print(\"\\nState-to-tag mapping:\")\n",
    "# for state, tag_idx in sorted(hmm.state_to_tag_mapping.items()):\n",
    "#     print(f\"State {state} maps to {idx_to_tag[tag_idx]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
