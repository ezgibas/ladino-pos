{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))  # Adjust the path accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Data and Prepare it for FastAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocess\n",
    "\n",
    "\n",
    "ladino_file_path = '../data/tatoeba.spa-lad.lad'\n",
    "spanish_file_path = '../data/tatoeba.spa-lad.spa'\n",
    "parallel_file_path = '../data/preprocessing/parallel_spa-lad.txt'\n",
    "spanish_pos_file_path = '../data/preprocessing/spanish_pos.txt'\n",
    "weak_dataset_file_path = '../data/weak/ladino-pos.txt'\n",
    "\n",
    "load_data = True # set True if you don't have the preprocessed files, otherwise just load them to save time\n",
    "if not load_data:\n",
    "    preprocess(ladino_file_path, spanish_file_path, parallel_file_path, spanish_pos_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align Tokens using FastAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'fast_align' already exists and is not an empty directory.\n",
      "mkdir: cannot create directory ‘build’: File exists\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/clab/fast_align.git\n",
    "!cd fast_align && mkdir build && cd build && cmake .. && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      "expected target length = source length * 1.04245\n",
      "ITERATION 1\n",
      "  log_e likelihood: -74562.3\n",
      "  log_2 likelihood: -107571\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.226178\n",
      "       size counts: 49\n",
      "ITERATION 2\n",
      "  log_e likelihood: -8788.09\n",
      "  log_2 likelihood: -12678.5\n",
      "     cross entropy: 3.52377\n",
      "        perplexity: 11.5017\n",
      "      posterior p0: 0.0112896\n",
      " posterior al-feat: -0.18629\n",
      "       size counts: 49\n",
      "  1  model al-feat: -0.150993 (tension=4)\n",
      "  2  model al-feat: -0.171948 (tension=3.29405)\n",
      "  3  model al-feat: -0.181367 (tension=3.00721)\n",
      "  4  model al-feat: -0.184724 (tension=2.90876)\n",
      "  5  model al-feat: -0.185805 (tension=2.87743)\n",
      "  6  model al-feat: -0.186141 (tension=2.86773)\n",
      "  7  model al-feat: -0.186244 (tension=2.86475)\n",
      "  8  model al-feat: -0.186276 (tension=2.86384)\n",
      "     final tension: 2.86356\n",
      "ITERATION 3\n",
      "  log_e likelihood: -5551.92\n",
      "  log_2 likelihood: -8009.73\n",
      "     cross entropy: 2.22616\n",
      "        perplexity: 4.67888\n",
      "      posterior p0: 0.0163447\n",
      " posterior al-feat: -0.183815\n",
      "       size counts: 49\n",
      "  1  model al-feat: -0.186286 (tension=2.86356)\n",
      "  2  model al-feat: -0.184578 (tension=2.91298)\n",
      "  3  model al-feat: -0.184054 (tension=2.92825)\n",
      "  4  model al-feat: -0.18389 (tension=2.93303)\n",
      "  5  model al-feat: -0.183839 (tension=2.93454)\n",
      "  6  model al-feat: -0.183822 (tension=2.93501)\n",
      "  7  model al-feat: -0.183817 (tension=2.93516)\n",
      "  8  model al-feat: -0.183816 (tension=2.93521)\n",
      "     final tension: 2.93523\n",
      "ITERATION 4\n",
      "  log_e likelihood: -5202.52\n",
      "  log_2 likelihood: -7505.66\n",
      "     cross entropy: 2.08606\n",
      "        perplexity: 4.24588\n",
      "      posterior p0: 0.0212052\n",
      " posterior al-feat: -0.182244\n",
      "       size counts: 49\n",
      "  1  model al-feat: -0.183815 (tension=2.93523)\n",
      "  2  model al-feat: -0.182742 (tension=2.96665)\n",
      "  3  model al-feat: -0.182403 (tension=2.97662)\n",
      "  4  model al-feat: -0.182295 (tension=2.97981)\n",
      "  5  model al-feat: -0.18226 (tension=2.98084)\n",
      "  6  model al-feat: -0.182249 (tension=2.98117)\n",
      "  7  model al-feat: -0.182245 (tension=2.98127)\n",
      "  8  model al-feat: -0.182244 (tension=2.9813)\n",
      "     final tension: 2.98132\n",
      "ITERATION 5 (FINAL)\n",
      "  log_e likelihood: -5112.9\n",
      "  log_2 likelihood: -7376.36\n",
      "     cross entropy: 2.05013\n",
      "        perplexity: 4.14143\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 49\n"
     ]
    }
   ],
   "source": [
    "! fast_align/build/fast_align -i ../data/preprocessing/parallel_spa-lad.txt -d -o -v > ../data/preprocessing/alignments.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer POS Tags from Spanish to Ladino using alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer tags for one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_pos_tag(ladino_sent, alignment, pos_tags):\n",
    "    \"\"\"Takes in a sentence from Ladino, alignments with Spanish, and the Spanish Part of Speech tags\n",
    "    and transfers the tags to the Ladino sentence\n",
    "    \"\"\"\n",
    "    ladino_pos = [\"UNK\"] * len(ladino_sent) # tags are unknown by default\n",
    "\n",
    "    for lad_idx, spa_idx in alignment:\n",
    "        if spa_idx < len(pos_tags):  # Ensure alignment is within bounds\n",
    "            ladino_pos[lad_idx] = pos_tags[spa_idx]\n",
    "    \n",
    "    return ladino_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create file to keep track of Ladino POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_preprocessed_data import load_alignments, load_parallel_data, load_spanish_pos\n",
    "alignments = load_alignments('../data/preprocessing/alignments.txt')\n",
    "ladino_tokens, spanish_tokens = load_parallel_data(parallel_file_path)\n",
    "spanish_pos = load_spanish_pos(spanish_pos_file_path)\n",
    "\n",
    "if not load_data:\n",
    "    with open('../data/weak/ladino-pos.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "            for sentence, alignment, pos_tags in zip(ladino_tokens, alignments, spanish_pos):\n",
    "                pos_tags = transfer_pos_tag(sentence, alignment, pos_tags)\n",
    "                formatted = \" \".join(f\"{word} ({tag})\" for word, tag in zip(sentence, pos_tags)) + \"\\n\"\n",
    "                f.write(formatted)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/weak/ladino-pos.txt\n"
     ]
    }
   ],
   "source": [
    "from load_preprocessed_data import load_ladino_pos\n",
    "\n",
    "ladino_tokens, tags_dict = load_ladino_pos(weak_dataset_file_path) # import ladino tokens into custom data definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671\n",
      "Eya (PRON)\n",
      "es (AUX)\n",
      "muy (ADJ)\n",
      "yakishikliya (NOUN)\n",
      ". (PUNCT)\n"
     ]
    }
   ],
   "source": [
    "print(len(ladino_tokens)) # check it worked\n",
    "for item in ladino_tokens[10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct the dataset so there are as few \"unknown\" tags as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping of each word to its most frequent tag (excluding \"unknown\")\n",
    "most_common_tags = {word: tags.most_common(1)[0][0] for word, tags in tags_dict.items() if tags}\n",
    "\n",
    "# replace \"unknown\" tags\n",
    "for sentence in ladino_tokens:\n",
    "    for token in sentence:\n",
    "        if token.get_pos() == \"UNK\" and token.get_word() in most_common_tags:\n",
    "            token.correct_pos(most_common_tags[token.get_word()])  # replace with most common tag\n",
    "\n",
    "# save corrected tags into file\n",
    "with open('../data/weak/ladino-pos.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "            for sentence in ladino_tokens:\n",
    "                formatted = \"\"\n",
    "                for token in sentence:\n",
    "                      formatted += str(token) + \" \"\n",
    "                formatted = formatted.strip() + \"\\n\"\n",
    "                f.write(formatted)\n",
    "            f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done, delete some variables to free up memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
