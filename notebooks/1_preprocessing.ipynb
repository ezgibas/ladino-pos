{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))  # Adjust the path accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Data and Prepare it for FastAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocess\n",
    "\n",
    "\n",
    "ladino_file_path = '../data/tatoeba.spa-lad.lad'\n",
    "spanish_file_path = '../data/tatoeba.spa-lad.spa'\n",
    "parallel_file_path = '../data/preprocessing/parallel_spa-lad.txt'\n",
    "spanish_pos_file_path = '../data/preprocessing/spanish_pos.txt'\n",
    "weak_dataset_file_path = '../data/weak/ladino-pos.txt'\n",
    "\n",
    "load_data = True # set True if you don't have the preprocessed files, otherwise just load them to save time\n",
    "if not load_data:\n",
    "    preprocess(ladino_file_path, spanish_file_path, parallel_file_path, spanish_pos_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align Tokens using FastAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/clab/fast_align.git\n",
    "!cd fast_align && mkdir build && cd build && cmake .. && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! fast_align/build/fast_align -i ../data/preprocessing/parallel_spa-lad.txt -d -o -v > ../data/preprocessing/alignments.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer POS Tags from Spanish to Ladino using alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer tags for one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_pos_tag(ladino_sent, alignment, pos_tags):\n",
    "    \"\"\"Takes in a sentence from Ladino, alignments with Spanish, and the Spanish Part of Speech tags\n",
    "    and transfers the tags to the Ladino sentence\n",
    "    \"\"\"\n",
    "    ladino_pos = [\"UNK\"] * len(ladino_sent) # tags are unknown by default\n",
    "\n",
    "    for lad_idx, spa_idx in alignment:\n",
    "        if spa_idx < len(pos_tags):  # Ensure alignment is within bounds\n",
    "            ladino_pos[lad_idx] = pos_tags[spa_idx]\n",
    "    \n",
    "    return ladino_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create file to keep track of Ladino POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_preprocessed_data import load_alignments, load_parallel_data, load_spanish_pos\n",
    "alignments = load_alignments('../data/preprocessing/alignments.txt')\n",
    "ladino_tokens, spanish_tokens = load_parallel_data(parallel_file_path)\n",
    "spanish_pos = load_spanish_pos(spanish_pos_file_path)\n",
    "\n",
    "if not load_data:\n",
    "    with open('../data/weak/ladino-pos.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "            for sentence, alignment, pos_tags in zip(ladino_tokens, alignments, spanish_pos):\n",
    "                pos_tags = transfer_pos_tag(sentence, alignment, pos_tags)\n",
    "                formatted = \" \".join(f\"{word} ({tag})\" for word, tag in zip(sentence, pos_tags)) + \"\\n\"\n",
    "                f.write(formatted)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_preprocessed_data import load_ladino_pos\n",
    "\n",
    "ladino_tokens, tags_dict = load_ladino_pos(weak_dataset_file_path) # import ladino tokens into custom data definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ladino_tokens)) # check it worked\n",
    "for item in ladino_tokens[10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct the dataset so there are as few \"unknown\" tags as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping of each word to its most frequent tag (excluding \"unknown\")\n",
    "most_common_tags = {word: tags.most_common(1)[0][0] for word, tags in tags_dict.items() if tags}\n",
    "\n",
    "# replace \"unknown\" tags\n",
    "for sentence in ladino_tokens:\n",
    "    for token in sentence:\n",
    "        if token.get_pos() == \"UNK\" and token.get_word() in most_common_tags:\n",
    "            token.correct_pos(most_common_tags[token.get_word()])  # replace with most common tag\n",
    "\n",
    "# save corrected tags into file\n",
    "with open('../data/weak/ladino-pos.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "            for sentence in ladino_tokens:\n",
    "                formatted = \"\"\n",
    "                for token in sentence:\n",
    "                      formatted += str(token) + \" \"\n",
    "                formatted = formatted.strip() + \"\\n\"\n",
    "                f.write(formatted)\n",
    "            f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done, delete some variables to free up memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
