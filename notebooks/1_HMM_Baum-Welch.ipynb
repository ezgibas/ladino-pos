{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))  # Adjust the path accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from load_data import *\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_brown_data(\"../data/brown-universal.txt\", split=0.8)\n",
    "tags = load_tags(\"../data/tags-universal.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} sentences in the training set.\".format(len(train)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition train so only a few of the samples are used for the initial probabilities\n",
    "train_sample = train[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "train_sentences = []\n",
    "for sentence in train_sample:\n",
    "    train_sentence = []\n",
    "    for token in sentence:\n",
    "        word = token.get_word()\n",
    "        if word == '``' or word == \"''\":\n",
    "            word = '\"'\n",
    "        words.append(word)\n",
    "        train_sentence.append(word)\n",
    "        \n",
    "            \n",
    "    train_sentences.append(train_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baum-Welch Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the code for train_em and baum_welch were written using NLTK's open-source code as a reference, which can be found here https://www.nltk.org/api/nltk.tag.hmm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaumWelch:\n",
    "    def __init__(self, tags, words):\n",
    "        self.states = tags\n",
    "        self.vocab = words\n",
    "        self.vocab_lookup = {word: i for i, word in enumerate(words)}\n",
    "        self.num_tags = len(self.states)  \n",
    "        self.vocab_size = len(words)  \n",
    "\n",
    "        # initialize uniform, can be initialized using initialize_probabilities() w/ other values\n",
    "        transition_probs = np.random.rand(self.num_tags, self.num_tags)\n",
    "        emission_probs = np.random.rand(self.num_tags, self.vocab_size)\n",
    "        initial_probs = np.random.rand(self.num_tags)\n",
    "\n",
    "        # normalize\n",
    "        transition_probs /= transition_probs.sum(axis=1, keepdims=True)  \n",
    "        emission_probs /= emission_probs.sum(axis=1, keepdims=True) \n",
    "        initial_probs /= initial_probs.sum() \n",
    "\n",
    "        self.transition_probs = np.log(transition_probs)\n",
    "        self.emission_probs = np.log(emission_probs)\n",
    "        self.initial_probs = np.log(initial_probs)\n",
    "    \n",
    "    def save_hmm(hmm, filename=\"../results/hmm_tagger-BW.pkl\"):\n",
    "        \"\"\"Save HMM to a file\"\"\"\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"transition_probs\": np.exp2(hmm.transition_probs),\n",
    "                \"emission_probs\": np.exp2(hmm.emission_probs),\n",
    "                \"initial_probs\": np.exp2(hmm.initial_probs),\n",
    "                \"vocab\": hmm.vocab,\n",
    "                \"states\": hmm.states\n",
    "            }, f)\n",
    "\n",
    "    def initialize_probabilities(self, transition, emission, initial, log=False):\n",
    "        \"\"\"initialize transition, emission, and initial probabilities.\n",
    "        log: boolean - if the probabilities fed in are in log space or not\n",
    "        \"\"\"\n",
    "        if not log:\n",
    "            self.transition_probs = np.log(transition)\n",
    "            self.emission_probs = np.log(emission)\n",
    "            self.initial_probs = np.log(initial)\n",
    "        else: \n",
    "            self.transition_probs = transition\n",
    "            self.emission_probs = emission\n",
    "            self.initial_probs = initial\n",
    "\n",
    "    def logsumexp2(self, arr):\n",
    "        max_ = max(arr)\n",
    "        return np.log2(np.sum(2 ** (arr - max_))) + max_\n",
    "\n",
    "\n",
    "    def train_em(self, sequences, max_iterations=100):\n",
    "        \"\"\"Train the HMM using the Expectation-Maximization algorithm.\"\"\"\n",
    "        # track convergence by looking at log probability\n",
    "        converged = False\n",
    "        prev_log_likelihood = None\n",
    "        iteration = 0\n",
    "        epsilon = 0.001\n",
    "\n",
    "        # compute expectations over all training sequences\n",
    "        # one loop = one forward-backward pass\n",
    "        while not converged and iteration < max_iterations:\n",
    "            # initialize probability tables to modify\n",
    "            acc_transitions_num = np.full((self.num_tags, self.num_tags), -np.inf) \n",
    "            acc_emissions_num = np.full((self.num_tags, self.vocab_size), -np.inf)\n",
    "            acc_initial_num = np.full(self.num_tags, -np.inf)\n",
    "\n",
    "            # track denominators for normalization\n",
    "            acc_transition_denom = np.full(self.num_tags, -np.inf)\n",
    "            acc_emission_denom = np.full(self.num_tags, -np.inf)\n",
    "            acc_initial_denom = -np.inf\n",
    "\n",
    "            log_likelihood = 0\n",
    "            for sequence in sequences:\n",
    "                if len(sequence) <= 1:\n",
    "                    log_prob_sequence = 1 # random value so that it doesn't falsely converge\n",
    "                    continue\n",
    "                # values for this sequence\n",
    "                (log_prob_sequence, \n",
    "                 seq_acc_transitions_num, \n",
    "                 seq_acc_emissions_num, \n",
    "                 seq_acc_transition_denom, \n",
    "                 seq_acc_emission_denom,\n",
    "                 seq_acc_initial_num, \n",
    "                 seq_acc_initial_denom) = self.baum_welch(sequence)\n",
    "                \n",
    "\n",
    "                for i in range(self.num_tags):\n",
    "                    # print(f\"{seq_acc_emissions_num[i]} - {log_prob_sequence}\")\n",
    "                    acc_transitions_num[i] = np.logaddexp2(acc_transitions_num[i], seq_acc_transitions_num[i] - log_prob_sequence)\n",
    "                    acc_emissions_num[i] = np.logaddexp2(acc_emissions_num[i], seq_acc_emissions_num[i] - log_prob_sequence)\n",
    "                \n",
    "                acc_transition_denom = np.logaddexp2(acc_transition_denom, seq_acc_transition_denom - log_prob_sequence)\n",
    "                acc_emission_denom = np.logaddexp2(acc_emission_denom, seq_acc_emission_denom - log_prob_sequence)\n",
    "\n",
    "                acc_initial_num = np.logaddexp2(acc_initial_num, seq_acc_initial_num - log_prob_sequence)\n",
    "                acc_initial_denom = np.logaddexp2(acc_initial_denom, seq_acc_initial_denom - log_prob_sequence)\n",
    "\n",
    "                log_likelihood += log_prob_sequence\n",
    "                \n",
    "            # update the transition and output probability values\n",
    "            for i in range(self.num_tags):\n",
    "                # print(f\"transition: {acc_transitions_num[i]} - {acc_transition_denom[i]}\")\n",
    "                # print(f\"emission: {acc_emissions_num[i]} - {acc_emission_denom[i]}\")\n",
    "                logprob_trans_i = acc_transitions_num[i] - acc_transition_denom[i]\n",
    "                logprob_ems_i = acc_emissions_num[i] - acc_emission_denom[i]\n",
    "                # replace any -inf with a value for stability\n",
    "                logprob_ems_i[np.isinf(logprob_ems_i) & (logprob_ems_i < 0)] = np.log(1e-10)\n",
    "                logprob_trans_i[np.isinf(logprob_trans_i) & (logprob_trans_i < 0)] = np.log(1e-10)\n",
    "\n",
    "                logprob_trans_i -= self.logsumexp2(logprob_trans_i)\n",
    "                logprob_ems_i -= self.logsumexp2(logprob_ems_i)\n",
    "\n",
    "\n",
    "                # transition probabilities\n",
    "                for j in range(self.num_tags):\n",
    "                    self.transition_probs[i, j] = logprob_trans_i[j]\n",
    "                # emission probabilities\n",
    "                for k in range(self.vocab_size):\n",
    "                    self.emission_probs[i, k] = logprob_ems_i[k]\n",
    "            # initial probabilities\n",
    "            logprob_initial = acc_initial_num - acc_initial_denom\n",
    "            # replace -inf for numerical stability\n",
    "            logprob_initial[np.isinf(logprob_initial) & (logprob_initial < 0)] = np.log(1e-10)\n",
    "            # normalize\n",
    "            logprob_initial -= self.logsumexp2(logprob_initial)\n",
    "\n",
    "            \n",
    "            # test for convergence\n",
    "            if iteration > 0 and abs(log_likelihood - prev_log_likelihood) < epsilon:\n",
    "                converged = True\n",
    "\n",
    "            print(\"iteration\", iteration, \"logprob\", log_likelihood)\n",
    "            iteration += 1\n",
    "            prev_log_likelihood = log_likelihood\n",
    "        return self\n",
    "\n",
    "    def baum_welch(self, sequence):  \n",
    "        \"\"\"One forward-backward pass\"\"\"    \n",
    "        # forward and backward probabilities\n",
    "        alpha = self.forward(sequence)\n",
    "        beta = self.backward(sequence)\n",
    "\n",
    "        log_prob_sequence = self.logsumexp2(alpha[-1][:])\n",
    "\n",
    "        # initialize probability tables to modify\n",
    "        acc_transition_num = np.full((self.num_tags, self.num_tags), -np.inf) \n",
    "        acc_emission_num = np.full((self.num_tags, self.vocab_size), -np.inf)\n",
    "        acc_inital_num = np.full(self.num_tags, -np.inf)\n",
    "\n",
    "        # track denominators for normalization\n",
    "        acc_transition_denom = np.full(self.num_tags, -np.inf)\n",
    "        acc_emission_denom = np.full(self.num_tags, -np.inf)\n",
    "        acc_initial_denom = -np.inf\n",
    "\n",
    "        for t in range(len(sequence)):\n",
    "            word = sequence[t]\n",
    "            next_word = None\n",
    "            if t < len(sequence) - 1:\n",
    "                next_word = sequence[t+1]\n",
    "                next_word_idx = self.vocab_lookup.get(next_word)\n",
    "                next_prob = self.emission_probs[:, next_word_idx:next_word_idx+1]\n",
    "            xi = self.vocab_lookup.get(word)\n",
    "\n",
    "            gamma = alpha[t] + beta[t]    \n",
    "\n",
    "            if t == 0:\n",
    "                acc_inital_num = gamma\n",
    "                acc_initial_denom = self.logsumexp2(gamma)   \n",
    "\n",
    "            if t < len(sequence) - 1:\n",
    "                numer_sum = self.transition_probs + next_prob + beta[t+1] + alpha[t].reshape(self.num_tags, 1)\n",
    "                acc_transition_num = np.logaddexp2(acc_transition_num, numer_sum)\n",
    "                acc_transition_denom = np.logaddexp2(acc_transition_denom, gamma)\n",
    "            else:\n",
    "                acc_emission_denom = np.logaddexp2(acc_transition_denom, gamma)\n",
    "            \n",
    "            acc_emission_num[:, xi] = np.logaddexp2(acc_emission_num[:, xi], gamma)\n",
    "        \n",
    "        return (log_prob_sequence,\n",
    "                acc_transition_num,\n",
    "                acc_emission_num, \n",
    "                acc_transition_denom,\n",
    "                acc_emission_denom,\n",
    "                acc_inital_num,\n",
    "                acc_initial_denom)\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        \"\"\"Compute forward probabilities (alpha)\"\"\"\n",
    "        sent_length = len(sequence)\n",
    "        alpha = np.full((sent_length, self.num_tags), -np.inf)\n",
    "\n",
    "        # initialization step\n",
    "        first_word = sequence[0]\n",
    "        word_idx = self.vocab_lookup.get(first_word) if first_word in self.vocab else -1\n",
    "\n",
    "        if word_idx >= 0: # if word is in vocabulary\n",
    "            alpha[0, :] = self.initial_probs + self.emission_probs[:, word_idx]\n",
    "        else:\n",
    "            alpha[0, :] = self.initial_probs + np.log2(1e-6)\n",
    "\n",
    "        # Recursion step\n",
    "        for t in range(1, sent_length):\n",
    "            word = sequence[t]\n",
    "            word_idx = self.vocab_lookup.get(word) if word in self.vocab else -1\n",
    "            for j in range(self.num_tags):\n",
    "                output_prob = 0\n",
    "                alpha_sum = self.logsumexp2(alpha[t-1] + self.transition_probs[:, j])\n",
    "                if word_idx >= 0: # if word is in vocabulary\n",
    "                    output_prob = self.emission_probs[j, word_idx]\n",
    "                else:\n",
    "                    output_prob = np.log2(1e-6)\n",
    "\n",
    "                alpha[t, j] = alpha_sum + output_prob\n",
    "        # replace any -inf for numerical stability\n",
    "        alpha[np.isinf(alpha) & (alpha < 0)] = np.log(1e-10)\n",
    "        return alpha\n",
    "    \n",
    "    def backward(self, sequence):\n",
    "        \"\"\"Compute backward probabilities (beta)\"\"\"\n",
    "        sent_length = len(sequence)\n",
    "        beta = np.full((sent_length, self.num_tags), -np.inf)\n",
    "\n",
    "        # initialize\n",
    "        last_word = sequence[-1]\n",
    "        word_idx = self.vocab_lookup.get(last_word) if last_word in self.vocab else -1\n",
    "\n",
    "        beta[-1, :] = self.initial_probs + np.log2(1e-6)\n",
    "\n",
    "        # recursion\n",
    "        for t in range(sent_length - 2, -1, -1):\n",
    "            word = sequence[t+1]  # emission (word) at t\n",
    "            word_idx =  self.vocab_lookup.get(word) if word in self.vocab else -1\n",
    "            for j in range(self.num_tags):\n",
    "                # P(transition from j -> any) + beta(t + 1 -> any)\n",
    "                # note: multiplication is addition in log space\n",
    "                beta_sum = self.transition_probs[j, :] + beta[t + 1] \n",
    "                # add P(next evidence | any)\n",
    "                if word_idx >= 0: # if word is in vocabulary\n",
    "                   beta_sum += self.emission_probs[j, word_idx]\n",
    "                else:\n",
    "                    beta_sum += np.log2(1e-6)\n",
    "                    \n",
    "                beta[t, j] = self.logsumexp2(beta_sum)\n",
    "\n",
    "        beta[np.isinf(beta) & (beta < 0)] = np.log(1e-10)\n",
    "        return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_tagger = BaumWelch(tags, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(train_sentences)\n",
    "batch_size = 50\n",
    "batch_start = 0\n",
    "batch_end = batch_size\n",
    "done = False\n",
    "while not done:\n",
    "    bw_tagger.train_em(train_sentences[batch_start:batch_end], max_iterations=10)\n",
    "    batch_start = batch_end\n",
    "    if batch_start >= num_samples:\n",
    "        done = True\n",
    "    else:\n",
    "        batch_end = min(batch_start + batch_size, num_samples)\n",
    "    if batch_end == num_samples:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_tagger.save_hmm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
