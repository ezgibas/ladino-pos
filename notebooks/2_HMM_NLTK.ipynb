{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))  # Adjust the path accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from load_data import *\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../data/brown-universal.txt\"\n",
    "tags_file = \"../data/tags-universal.txt\"\n",
    "predictions_file = \"../results/pred-viterbi-tags.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_tagged_sentences(data_file, split=0.8)\n",
    "tags = load_tags(tags_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} sentences in the training set.\".format(len(train)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition train so only a few of the samples are used for the initial probabilities\n",
    "train_sample = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train HMM with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def laplace_smoothing(freq_dist, bins):\n",
    "    return nltk.LaplaceProbDist(freq_dist, bins)\n",
    "\n",
    "def trainer(data):\n",
    "    trainer = nltk.tag.hmm.HiddenMarkovModelTrainer()\n",
    "    hmm_tagger = trainer.train_supervised(data, estimator=laplace_smoothing)\n",
    "    return hmm_tagger\n",
    "\n",
    "data = [[(token.get_word(), token.get_pos()) for token in sentence] for sentence in train_sample]\n",
    "hmm_tagger = trainer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the hmm so we don't have to train every time\n",
    "import pickle\n",
    "\n",
    "# Save the trained HMM model to a file\n",
    "with open(\"../results/hmm_tagger.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hmm_tagger, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using trained HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO IMPORT FROM SAVED FILE FOR LATER USES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get parameters of trained HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model parameters\n",
    "tags = hmm_tagger._states  # Set of all possible tags\n",
    "words = hmm_tagger._symbols  # Set of all possible words\n",
    "\n",
    "# convert probability distributions of HMM to dictionaries\n",
    "transitions = {}\n",
    "for prev_state in hmm_tagger._transitions:\n",
    "    transitions[prev_state] = {}\n",
    "    for next_state in tags:\n",
    "        transitions[prev_state][next_state] = hmm_tagger._transitions[prev_state].prob(next_state)\n",
    "\n",
    "emissions = {}\n",
    "for state in tags:\n",
    "    emissions[state] = {}\n",
    "    for word in words:\n",
    "        emissions[state][word] = hmm_tagger._outputs[state].prob(word)\n",
    "\n",
    "initial = {}\n",
    "for state in tags:\n",
    "    initial[state] = hmm_tagger._priors.prob(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, tags, transitions, emissions, initial):\n",
    "        self.tags = tags\n",
    "        self.transitions = transitions\n",
    "        self.emissions = emissions\n",
    "        self.initial = initial\n",
    "\n",
    "    def viterbi(self, sequence):\n",
    "        \"\"\"\n",
    "        Predict the Part of Speech tags for a given sentence using the Viterbi algorithm\n",
    "        \"\"\"\n",
    "        tags = self.tags\n",
    "        transitions = self.transitions\n",
    "        emissions = self.emissions\n",
    "        initial = self.initial\n",
    "\n",
    "        # viterbi matrix\n",
    "        # V[t][i] = value of path with the highest probability that accounts for the first t observations\n",
    "        V = [{}] \n",
    "        # path matrix\n",
    "        # path[t][i] = path w/ highest probability that accounts for first t observations\n",
    "        path = [{}]\n",
    "\n",
    "        # i.e. V is the max(), path is the argmax() \n",
    "        \n",
    "        # initialize first step\n",
    "        for state in tags:\n",
    "            # handle OOV words w/ small probability\n",
    "            emission_prob = emissions[state].get(sequence[0], 1e-5)\n",
    "            V[0][state] = initial[state] * emission_prob\n",
    "            path[0][state] = [state]\n",
    "        \n",
    "        # recursion\n",
    "        for t in range(1, len(sequence)):\n",
    "            V.append({})\n",
    "            path.append({})\n",
    "            \n",
    "            for cur_state in tags:\n",
    "                # handle OOV\n",
    "                emission_prob = emissions[cur_state].get(sequence[t], 1e-5)\n",
    "                \n",
    "                # initialize max, argmax to nothing\n",
    "                max_prob = float('-inf')\n",
    "                max_state = None\n",
    "                \n",
    "                # get max, argmax of V[t-1][i]*transitions[i][j] over all states i\n",
    "                for prev_state in tags: # for all states i\n",
    "                    # smoothing for missing transitions\n",
    "                    transition_prob = transitions[prev_state].get(cur_state, 1e-5)\n",
    "\n",
    "                    # V[t-1][i]*transitions[i][j]\n",
    "                    prob = V[t-1][prev_state] * transition_prob * emission_prob\n",
    "                    \n",
    "                    # max, argmax\n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        max_state = prev_state\n",
    "                \n",
    "                # V[t][j] = max(V[t-1][i]*transitions[i][j])*emissions[j][observation_t]\n",
    "                V[t][cur_state] = max_prob\n",
    "                # path[t][j] = argmax(V[t-1][i]*transitions[i][j])*emissions[j][observation_t]\n",
    "                path[t][cur_state] = path[t-1][max_state] + [cur_state]\n",
    "                    \n",
    "\n",
    "        # termination + backtracking\n",
    "        T = len(sequence)-1 # T = last time-step\n",
    "\n",
    "        best_final_state = max(V[T], key=V[T].get)\n",
    "\n",
    "        best_path = path[T][best_final_state] # path stores completes paths, so just access the last one\n",
    "\n",
    "        return list(zip(sequence, best_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for saving predictions into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tagged_sentence(tagged_sentence):\n",
    "    return \" \".join([f\"{word} ({tag})\" for word, tag in tagged_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test \n",
    "test_sample = [[token.get_word() for token in sentence] for sentence in test_sample]\n",
    "tagger = Predictor(tags, transitions, emissions, initial)\n",
    "\n",
    "with open(predictions_file, 'w') as f:\n",
    "    for sentence in test_sample:\n",
    "        # Tag the sentence\n",
    "        tagged_sentence = tagger.viterbi(sentence)\n",
    "        \n",
    "        # Format and write to file\n",
    "        formatted_sentence = format_tagged_sentence(tagged_sentence)\n",
    "        f.write(formatted_sentence + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
